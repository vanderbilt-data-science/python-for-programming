{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python and pandas for deep learning\n",
    "> Using Python and pandas for working with your files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we examine how one can work with files, particularly with pandas to get the data in a desired format for usage with deep learning packages.  If you're working with Google Colab without Google Drive mounted, keep in mind that you will not be able to use `glob` as one normally would.  As a workaround, we'll load the data files directly from the source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data science packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "#import file helper packages\n",
    "import glob\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab helper \n",
    "colab=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example scenario\n",
    "You're working with the text of number of magazine articles, written by a number of authors, whose names are not included in the text.  The magazine articles have unique IDs, which are given by their filenames.  \n",
    "\n",
    "You also have an accompanying CSV file with details about all of the authors, including their name, age, years of employment as a journalist, college major, and IDs corresponding to the articles that they've written.  \n",
    "\n",
    "You're interested in learning whether their college major affects the sentiments of their articles.  The only data that you have access to is gathered on a particular day over a set of journalists and articles.  You've decided that the first crack you'll take about understanding this relationship is to separate the journalists (or articles) into groups according to their college major, and count the sentiments for all the corresponding articles for each group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access data\n",
    "The first thing you'll need to do is get some data, and be able to load it for processing with Python.  For this, you can generally use the `glob` package on a local computer or somewhere that you have the files mounted.  If you're local, you can follow the steps below.  Otherwise, we'll implement a quick workaround for Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames=[]\n",
    "if colab is False:\n",
    "    filenames = glob.glob('./*/*.txt')\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble filenames\n",
    "A filename is a path to a file.  Here, we have the \"directory\" of the file with the list of short names of the file.  If we concatenate (add) these two parts together, we can have an entire filename for each path.  We could then use this to get the text!  Let's use a list comprehension to concatenate the strings together (e.g., using `+` between two strings).  You can use the string function `str()` to convert a numeric to a string.  \n",
    "\n",
    "**Take a minute and try to do this on your own.**  It may help to narrate what you're trying to do to yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = [551293, 373587, 597061, 434648, 532970,\n",
    "              520668, 209035, 830014, 671125, 893941,\n",
    "              479957, 541893, 836261, 244666, 696866,\n",
    "              332305, 930880, 297116, 542169, 272307]\n",
    "file_base = 'https://raw.githubusercontent.com/vanderbilt-data-science/python-for-deep-learning-workshop/master/workshop-files/'\n",
    "if colab:\n",
    "    filenames = #enter you own code here!\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get file contents\n",
    "We're going to be using the requests package to read from a remote file (the repo on GitHub).  You can use the `requests.get(insert-url-path-here.txt)` function to get the contents of a remote url.  Let's use a list comprehension to implement this functionality.  We'll do this together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read file contents\n",
    "if colab:\n",
    "    #how to read files here?\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Length of file contents list:', len(file_contents))\n",
    "file_contents[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read tabular data\n",
    "Here, we'll read the tabular data stored in the csv about the authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate file path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief assessment of tabular data\n",
    "Generally, you want to take a look at your data and make sure that it looks right.  We'll look at a few things below:\n",
    "* How many rows and columns are in the data?\n",
    "* How many null values does the data have?  (and what should I do about them?)\n",
    "* Are all of the article IDs unique?\n",
    "* What are the unique college majors and how many are there?\n",
    "We'll investigate these below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many rows and columns are there in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many null values does the data have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are all of the article IDs unique?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the unique college majors and how many are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returning to our original problem\n",
    "Now, we have the author information in a dataframe, a list of the full filenames, and a list of the corresponding text.  Consider the following scenario: we want to fine-tune a model (or directly use a model) using only \"experienced\" journalists with more than 5 years of experience.  How could I use these data structures to create a list of the texts corresponding ONLY to the journalists with more than 5 years of experience?\n",
    "### In-class breakout discussion\n",
    "In your breakout room document, identify a few ways that you could put together or use the dataframe, the list of filenames, and the list of corresponding text.  Don't write any code - just list the steps.  Remember that for all lists, you have the values of the lists and the indices.  Your filename list and your text list are collated (i.e., the index of the filename corresponds to the index of the text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary> <u> Click here for a potential solution.  Discuss with your group first!</u> </summary>\n",
    "    <pre>\n",
    "        <br>Potential Solution:<br>\n",
    "        #get all of the IDs for authors with more than 5 years of experience\n",
    "        exp_ids = author_df.query(\"years_of_journalism >5\")['article_id']  <br>\n",
    "        #turn it into a list (exp_list)\n",
    "        exp_list = exp_ids.tolist() <br>\n",
    "        #turn the file_names and file_contents into a dict\n",
    "        file_dict = dict(zip(files_list, file_contents)) <br>\n",
    "        #select the indices\n",
    "        res_text = [text for fnum, text in file_dict.items() if fnum in exp_list]\n",
    "        len(res_text)\n",
    "    </pre>\n",
    "   </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataframes from lists/dictionaries\n",
    "Maybe that wasn't the easiest method in the world.  There are many other ways to handle this, depending on the size of your data and your interests.  One solution could be to add a column in your dataframe which is the actual filename.  After you've done all of your selection, you can choose to load these files iteratively.  Alternately, if your text is a reasonable size, you could put all of it together in a single dataframe.  We'll try that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, create a dataframe from your filename list and your text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting data together: joining data\n",
    "Now, we can join the data together.  There are many different types of joins, but essentially a join will look for one or more key, match the key, and then bind the columns of both dataframes together where the key matches.  We'll do an outer join here, which will place NAs where there are no matches to the key value in the other table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Text info columns:\", tinfo_df.columns.tolist())\n",
    "print(\"Author columns:\", author_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the names of the keys in each dataframe that we want to join the tables on?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(full_df.head())\n",
    "print(full_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returning to the original question...\n",
    "How can we filter out \"inexperienced\" authors, and then select only the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-class Breakout Room Exercises\n",
    "Try this yourself!  In your groups, try the following:\n",
    "1. Get the filenames corresponding to authors who are younger than 55.  How many people are there?\n",
    "2. How many people are older than 60 and have less than 10 years of experience?  \n",
    "* Bonus. Get the list of texts for people whose college major was 'humanities'.  How many are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "\n",
    "display(young_fnames)\n",
    "print(len(young_fnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "\n",
    "display(inexp_df)\n",
    "print(len(inexp_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Results\n",
    "Now that we have all of our data together and are able to extract the text of any desired subset, let's do some modeling!  Just kidding, we'll do that next week.  For now, we'll just simulate the results using the following randomized function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_model(list_of_texts):\n",
    "    '''\n",
    "    Function sentiment_model: returns whether a text expresses positive or negative sentiment with confidence score\n",
    "        Inputs: a list of strings which correspond to individual texts\n",
    "        Returns: a list of dictionaries containing key:value pairs of 'label':'POSITIVE' or 'NEGATIVE' and 'score':float\n",
    "    '''\n",
    "    \n",
    "    text_res = []\n",
    "    \n",
    "    for text in list_of_texts:\n",
    "        \n",
    "        single_res = {}\n",
    "        \n",
    "        #decide whether it is positive or negative sentiment\n",
    "        if len(text) % 2 == 0:\n",
    "            single_res['label'] = 'POSITIVE'\n",
    "        else:\n",
    "            single_res['label'] = 'NEGATIVE'\n",
    "        \n",
    "        #decide on confidence\n",
    "        single_res['score'] = round(np.random.uniform(), 4)\n",
    "        \n",
    "        #append to list\n",
    "        text_res.append(single_res)\n",
    "  \n",
    "    #return the list\n",
    "    return text_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get modeling results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These seem like some pretty arbitrary choices for a workshop.  Are they?  Check out the HuggingFace documentation on sentiment analysis pipelines here: https://huggingface.co/transformers/quicktour.html#getting-started-on-a-task-with-a-pipeline and answer the following questions.  Make sure to add your answers to the breakout room document!\n",
    "### In-class Breakout Discussion\n",
    "* What is passed into the model?\n",
    "* What is returned by the model?\n",
    "* How does the example correspond to how one uses HuggingFace?\n",
    "\n",
    "## How do we combine these results with the original dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe from list of dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate dataframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's actually answer our question!\n",
    "Can we see a difference between the sentiments of articles based on college major?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we wanted to compute an aggregate score?\n",
    "Looking at negative and positive is a bit hard.  Maybe we could just calculate a score reflecting whether overall, the group tends to have positive reviews.  We can calculate this as `no_positive_reviews - no_negative_reviews`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What we've covered\n",
    "We've covered a lot of ground today!  We've discussed several things:\n",
    "* Reading in text files via Google Colab and locally via the glob package\n",
    "* Using lists, list comprehensions, and dictionaries to faciliate coding efforts\n",
    "* Using primitive data structures to create pandas dataframes\n",
    "* Appreciating the capabilites of pandas data structures for manipulation of data into desired formats\n",
    "* Grammar of data manipulation\n",
    "  - Select (using specific columns): `[]`\n",
    "  - Filter (removing some parts of the data while keeping others): `.query()`, `.loc()`, `.iloc()`\n",
    "  - Groupby (doing operations based on a group)\n",
    "  - Mutate (adding new columns onto the dataframe): `df['new_col']=values`\n",
    "* Example of HuggingFace sentiment-analysis model inference using demo function\n",
    "* Converting outputs to dataframes\n",
    "* Joining (`merge`) dataframes based on a key\n",
    "* Concatenating dataframes along an axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Exercises\n",
    "1.  Outline the advantages and disadvantages of using lists, dictionaries, and pandas dataframes for manipulating data.\n",
    "2.  Write your own implementation of selecting the text for inexperienced authors using the author dataframe, lists of text, and the filenames/ids.\n",
    "3.  We made one big dataframe with the author information alongside the text.  What if you didn't add the text as a column and only used the filename column?  How could you use this in a for loop or list comprehension to read your selected data into a list?  This is applicable for image analysis.\n",
    "4.  Repeat these exercises using the `.loc[]` method for filtering rather than `.query()`.\n",
    "5.  The model was very confident (the `score` column) for some predictions and less confident on others.  Suppose we want to identify all of the rows where the score is between 0.4 and 0.6 and change the label in `label` to `NEUTRAL`. Write some code that will implement this.  `.loc` will be your friend here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
